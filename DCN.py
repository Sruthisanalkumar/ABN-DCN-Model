# -*- coding: utf-8 -*-
"""dcn2022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1juGy6E6dh70u8Qn5fNpFlvyubSx5UpgE
"""
import fastai
from fastai.vision import *
from fastai.imports import *
# -*- coding: utf-8 -*-
"""Copy of DCN_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ll4Q8qMrdqYblNzqpabwRgNImYPv2hPA
"""
_all__ = ['DCNModel', 'dcn']
import math
import fastai
from fastai.vision import *
from fastai.basics import *
from fastai.imports import *

def conv_block(ni, nf, size=3, stride=1):
    for_pad = lambda s: s if s > 2 else 3
    return nn.Sequential(
        nn.Conv2d(ni, nf, kernel_size=size, stride=stride,
                  padding=(for_pad(size) - 1)//2, bias=False), 
        nn.BatchNorm2d(nf),
        nn.LeakyReLU(negative_slope=0.1, inplace=True)  
    )
def maxpooling():
    return nn.MaxPool2d(2, stride=2)
def conv_layer(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias:bool=None, is_1d:bool=False,
               norm_type:Optional[NormType]=NormType.Batch,  use_activ:bool=True, leaky:float=None,
               transpose:bool=False, init:Callable=nn.init.kaiming_normal_, self_attention:bool=False):
    "Create a sequence of convolutional (`ni` to `nf`), ReLU (if `use_activ`) and batchnorm (if `bn`) layers."
    if padding is None: padding = (ks-1)//2 if not transpose else 0
    bn = norm_type in (NormType.Batch, NormType.BatchZero)
    if bias is None: bias = not bn
    conv_func = nn.ConvTranspose2d if transpose else nn.Conv1d if is_1d else nn.Conv2d
    conv = init_default(conv_func(ni, nf, kernel_size=ks, bias=bias, stride=stride, padding=padding), init)
    if   norm_type==NormType.Weight:   conv = weight_norm(conv)
    elif norm_type==NormType.Spectral: conv = spectral_norm(conv)
    layers = [conv]
    if use_activ: layers.append(nn.LeakyReLU(inplace=True))
    if bn: layers.append((nn.BatchNorm1d if is_1d else nn.BatchNorm2d)(nf))
    if self_attention: layers.append(SelfAttention(nf))
    return nn.Sequential(*layers)
# DCN Basic block
class DCNBlock(nn.Module):
    def __init__(self,ni,nf):
        super(DCNBlock, self).__init__()
        self.layer1 = conv_block(ni,nf)
        self.layer2 = conv_block(nf,ni,size=1)
        self.layer3 = conv_block(ni,nf)

    def forward(self, x):
        # residual = x
        out = self.layer1(x)
        out = self.layer2(out)
        out=self.layer3(out)
        return out

class DCNModel(nn.Module):
    def __init__(self, block=DCNBlock, num_classes=2):
        super(DCNModel, self).__init__()

        self.num_classes = num_classes

        self.conv1 = conv_block(3, 8)
        self.max1 = maxpooling()
        self.conv2 = conv_block(8, 16)
        self.max2 = maxpooling()
        self.residual_block1 = self.make_layer(block, in_channels=16,out_channels=32, num_blocks=1)
        self.max3 = maxpooling()
        self.residual_block2 = self.make_layer(block, in_channels=32,out_channels=64, num_blocks=1)
        self.max4 = maxpooling()
        self.residual_block3 = self.make_layer(block, in_channels=64,out_channels=128, num_blocks=1)
        self.max5 = maxpooling()
        self.residual_block4 = self.make_layer(block, in_channels=128,out_channels=256, num_blocks=1)
        self.conv3 = conv_block(256, 128, size=1)
        self.conv4 = conv_block(128, 256)
        self.att_con= conv_layer(256,2)
        self.bn_att4 = nn.BatchNorm2d(2)
        self.att_conv2=nn.Conv2d(2, num_classes, kernel_size=1, padding=0,bias=False)
        self.bn_att =nn.BatchNorm2d(2)
        self.att_conv3  = nn.Conv2d(num_classes, 1, kernel_size=3, padding=1,bias=False)
        self.bn_att3 = nn.BatchNorm2d(1)
        self.att_gap = nn.AvgPool2d(11)
        self.sigmoid = nn.Sigmoid()
        self.conv5= conv_layer(256, 2)
        self.avgpool = nn.AvgPool2d(2, stride=1)
        self.fc = nn.Linear(338, num_classes)
        self.relu = nn.ReLU(inplace=True)
     # Weight initialization
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
    def make_layer(self, block, in_channels, out_channels, num_blocks):
        layers = []
        for i in range(0, num_blocks):
            layers.append(block(in_channels,out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.max1(x)
        x = self.conv2(x)
        x = self.max2(x)
        x = self.residual_block1(x)
        x = self.max3(x)
        x = self.residual_block2(x)
        x = self.max4(x)
        x = self.residual_block3(x)
        x = self.max5(x)
        x = self.residual_block4(x)
        x = self.conv3(x)
        x = self.conv4(x)
        # x = self.conv5(x)
        fe = x                 #256,256,13,13 shape
        print(fe.shape)
        ax=self.bn_att4(self.att_con(x)) # 256,2,13,13
        print(ax.shape)
        ax=self.relu(self.bn_att(self.att_conv2(ax))) #256,256,13,13 wrong?
        print(ax.shape)
        bs, cs, ys, xs = ax.shape
        self.att = self.sigmoid(self.bn_att3(self.att_conv3(ax)))#256,1,13,13
        print(self.att.shape)
        ax = self.att_gap(ax)
        ax = ax.view(ax.size(0), -1)
        rx = x * self.att
        rx = rx + x
        per = rx
        rx = self.conv5(rx)
        rx = rx.view(rx.size(0), -1)
        rx = self.fc(rx)
        return ax, rx, [self.att, fe, per]

def dcn(block,num_classes):
    """Constructs a DCN model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    return DCNModel(DCNBlock,num_classes=2)


